{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ca2ac08",
   "metadata": {},
   "source": [
    "# Train and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2c9d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$('<div id=\"toc\"></div>').css({position: 'fixed', top: '120px', left: 0}).appendTo(document.body);\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17630718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5549e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcb18d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change matplotlib parameters\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "plt.rcParams.update({'font.size': 30})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceab410",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89834b1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527a748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import random\n",
    "import copy\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForPreTraining, BertModel, AutoTokenizer, BertForSequenceClassification, RobertaForSequenceClassification\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "random_seed = 0\n",
    "torch.manual_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2137e1b7",
   "metadata": {},
   "source": [
    "## Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bd0143",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_test_flag = True\n",
    "\n",
    "# fold\n",
    "k=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683c4c2f",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ee0669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(tweets):\n",
    "    normalized_tweets = []\n",
    "    for text in tweets:\n",
    "        text = text.replace('&amp;', '&')\n",
    "        text = text.replace('\\xa0', '')\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = \" \".join(text.split())\n",
    "        normalized_tweets.append(text)\n",
    "    return normalized_tweets\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63a8b42",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7716fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f413dffc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#data_path = '../data/covid-latent/'\n",
    "#data_path = '../data/covid-latent/undersampling/'\n",
    "#data_path = '../data/stance-detection-in-covid-19-tweets/stay_at_home_orders/' #face_masks, school_closures, stay_at_home_orders, fauci\n",
    "#data_path = '../data/russian-troll-tweets/'\n",
    "#data_path = '../data/COVIDSenti/'\n",
    "#data_path = '../data/birdwatch/'\n",
    "data_path = '../data/mediaeval22/old_task1/'\n",
    "\n",
    "filelist = os.listdir(data_path)\n",
    "df_list = [pd.read_csv(data_path+file) for file in filelist if 'fold' in file]\n",
    "\n",
    "\n",
    "test_df = df_list[k]\n",
    "\n",
    "train_df = pd.concat(df_list[:k]+df_list[k+1:])\n",
    "test_df = pd.concat([train_df, test_df])\n",
    "\n",
    "\n",
    "tw_train = train_df['tweet'].tolist()\n",
    "tw_test = test_df['tweet'].tolist()\n",
    "\n",
    "# tw_train = train_df['content'].tolist()\n",
    "# tw_test = test_df['content'].tolist()\n",
    "# ids_test = test_df['tweet'].tolist()\n",
    "\n",
    "\n",
    "if normalize_test_flag:\n",
    "    tw_train = normalize_text(tw_train)\n",
    "    tw_test = normalize_text(tw_test)\n",
    "\n",
    "#emotion\n",
    "train_df['emotion'][train_df['emotion'].isna()]='N'\n",
    "labels_train = train_df['emotion'].to_numpy()\n",
    "labels_train[labels_train=='N']=0\n",
    "labels_train[labels_train=='H']=1\n",
    "labels_train[labels_train=='A']=2\n",
    "labels_train[labels_train=='S']=3\n",
    "labels_train[labels_train=='F']=4\n",
    "labels_train = labels_train.tolist()\n",
    "\n",
    "#sentiment\n",
    "#labels_train = train_df['label'].to_numpy()\n",
    "#labels_train[labels_train=='neu']=1\n",
    "#labels_train[labels_train=='pos']=2\n",
    "#labels_train[labels_train=='neg']=0\n",
    "#labels_train = labels_train.tolist()\n",
    "\n",
    "#political bias\n",
    "#labels_train = train_df['account_category'].to_numpy()   \n",
    "#labels_train[labels_train==\"Unknown\"]=1\n",
    "#labels_train[labels_train==\"NonEnglish\"]=1\n",
    "#labels_train[labels_train==\"Commercial\"]=1\n",
    "#labels_train[labels_train==\"NewsFeed\"]=1\n",
    "#labels_train[labels_train==\"HashtagGamer\"]=1\n",
    "#labels_train[labels_train==\"Fearmonger\"]=1\n",
    "#labels_train[labels_train==\"LeftTroll\"]=0\n",
    "#labels_train[labels_train==\"RightTroll\"]=2\n",
    "#labels_train = labels_train.tolist()\n",
    "\n",
    "emotion\n",
    "test_df['emotion'][test_df['emotion'].isna()]='N'\n",
    "labels_test = test_df['emotion'].to_numpy()\n",
    "labels_test[labels_test=='N']=0\n",
    "labels_test[labels_test=='H']=1\n",
    "labels_test[labels_test=='A']=2\n",
    "labels_test[labels_test=='S']=3\n",
    "labels_test[labels_test=='F']=4\n",
    "labels_test = labels_test.tolist()\n",
    "\n",
    "#sentiment\n",
    "# labels_test = test_df['label'].to_numpy()\n",
    "# labels_test[labels_test=='neu']=1\n",
    "# labels_test[labels_test=='pos']=2\n",
    "# labels_test[labels_test=='neg']=0\n",
    "# labels_test = labels_test.tolist()\n",
    "\n",
    "#political bias\n",
    "# labels_test = test_df['account_category'].to_numpy()   \n",
    "# labels_test[labels_test==\"Unknown\"]=1\n",
    "# labels_test[labels_test==\"NonEnglish\"]=1\n",
    "# labels_test[labels_test==\"Commercial\"]=1\n",
    "# labels_test[labels_test==\"NewsFeed\"]=1\n",
    "# labels_test[labels_test==\"HashtagGamer\"]=1\n",
    "# labels_test[labels_test==\"Fearmonger\"]=1\n",
    "# labels_test[labels_test==\"LeftTroll\"]=0\n",
    "# labels_test[labels_test==\"RightTroll\"]=2\n",
    "# labels_test = labels_test.tolist()\n",
    "\n",
    "ids_test = [i for i in range(0, len(test_df))]\n",
    "\n",
    "#labels_train = [[l-1 for l in L] for L in labels_train]\n",
    "#labels_test = [[l-1 for l in L] for L in labels_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f57cca3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights = [len(labels_train)/w for w in [labels_train.count(a) for a in range(0, 3)]]\n",
    "weights = torch.FloatTensor(weights).to(device)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a815a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
    "\n",
    "tokenized_input = tokenizer(tw_train)\n",
    "\n",
    "m = 0\n",
    "for tokens in tokenized_input['input_ids']:\n",
    "    if len(tokens)>m:\n",
    "        m=len(tokens)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633285bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128 # < m some tweets will be truncated\n",
    "\n",
    "tokenized_input = tokenizer(tw_train, max_length=MAX_LEN, padding='max_length', truncation=True)\n",
    "tokenized_test = tokenizer(tw_test, max_length=MAX_LEN, padding='max_length', truncation=True)\n",
    "    \n",
    "train_input_ids, train_token_type_ids, train_attention_mask = tokenized_input['input_ids'], tokenized_input['token_type_ids'], tokenized_input['attention_mask']\n",
    "test_input_ids, test_token_type_ids, test_attention_mask = tokenized_test['input_ids'], tokenized_test['token_type_ids'], tokenized_test['attention_mask']\n",
    "\n",
    "train_token_type_ids = torch.tensor(train_token_type_ids)\n",
    "test_token_type_ids = torch.tensor(test_token_type_ids)\n",
    "    \n",
    "    \n",
    "train_labels = labels_train\n",
    "test_labels = labels_test\n",
    "\n",
    "\n",
    "# Convert to torch tensor\n",
    "train_input_ids = torch.tensor(train_input_ids)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_attention_mask = torch.tensor(train_attention_mask)\n",
    "\n",
    "test_input_ids = torch.tensor(test_input_ids)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "test_attention_mask = torch.tensor(test_attention_mask)\n",
    "test_ids = torch.tensor(ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abcec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12 #\n",
    "\n",
    "train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels, train_token_type_ids)\n",
    "test_data = TensorDataset(test_input_ids, test_attention_mask, test_labels, test_token_type_ids, test_ids)\n",
    "\n",
    "    \n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f3ad8b",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657e5a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovidTwitterBertClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.bert = BertForPreTraining.from_pretrained('digitalepidemiologylab/covid-twitter-bert-v2')    \n",
    "        self.bert.cls.seq_relationship = nn.Linear(1024, n_classes)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids, input_mask):\n",
    "        outputs = self.bert(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = input_mask)\n",
    "\n",
    "        logits = outputs[1]\n",
    "        \n",
    "        return logits  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137d7cb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = CovidTwitterBertClassifier(5) # 5 for emotion and 3 for sentiment and political bias\n",
    "    \n",
    "    \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb60b67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer_grouped_parameters\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5,\n",
    "                  #lr=3e-5,\n",
    "                  weight_decay = 0.01)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, patience=4, factor=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b509c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight = weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ad823b",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89243993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "\n",
    "best_F1 = 0\n",
    "best_ACC = 0\n",
    "best_loss = 999\n",
    "best_acc = 0\n",
    "best_state_dict = model.state_dict()\n",
    "best_epoch = 0\n",
    "\n",
    "for e in trange(0, epochs, position=0, leave=True):\n",
    "\n",
    "    # Training\n",
    "    print('Starting epoch ', e)\n",
    "    model.train()\n",
    "    \n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        b_input_ids, b_input_mask, b_labels, b_token_type_ids = batch\n",
    "        \n",
    "        b_labels = b_labels.float()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(b_input_ids, b_token_type_ids, b_input_mask)\n",
    "                \n",
    "        loss = criterion(logits, b_labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "    \n",
    "    # eval\n",
    "    \n",
    "    logits_full = []\n",
    "    ground_truth_full = []\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    steps=0\n",
    "    for step, batch in enumerate(tqdm(test_dataloader)):\n",
    "\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        b_input_ids, b_input_mask, b_labels, b_token_type_ids, b_ids = batch\n",
    "            \n",
    "        b_labels = b_labels.float()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            logits = model(b_input_ids, b_token_type_ids, b_input_mask)\n",
    "            loss = criterion(logits, b_labels.long())\n",
    "    \n",
    "\n",
    "\n",
    "        logits = logits.detach().cpu().tolist()\n",
    "        logits_full.extend(logits)\n",
    "        ground_truth = b_labels.detach().cpu().tolist()\n",
    "        ground_truth_full.extend(ground_truth)\n",
    "        \n",
    "        steps+=1\n",
    "        eval_loss+=loss.detach().item()\n",
    "        \n",
    "    \n",
    "    scheduler.step(eval_loss/steps)\n",
    "    LOSS = eval_loss/steps\n",
    "    F1 = metrics.f1_score(np.array(logits_full).argmax(axis=1), np.array(ground_truth_full), average='micro')\n",
    "    ACC = metrics.accuracy_score(np.array(logits_full).argmax(axis=1), np.array(ground_truth_full))\n",
    "    \n",
    "    if F1> best_F1:\n",
    "        best_loss = LOSS\n",
    "        best_F1 = F1\n",
    "        best_ACC = ACC\n",
    "        best_state_dict = copy.deepcopy(model.state_dict())\n",
    "        best_epoch = e\n",
    "    \n",
    "    print(\"\\t Eval loss: {}\".format(LOSS))\n",
    "    print(\"\\t Eval F1: {}\".format(F1))\n",
    "    print(\"\\t Eval ACC: {}\".format(ACC))\n",
    "    print(\"---\"*25)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd4526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best epoch\", best_epoch)\n",
    "print(\"\\t Eval loss: {}\".format(best_loss))\n",
    "print(\"\\t Eval F1: {}\".format(best_F1))\n",
    "print(\"---\"*25)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681b07c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_state_dict, '../data/covid-latent/models/emotion_undersampling_CV'+str(k)+'_e'+str(best_epoch)+'_'+str(round(best_F1, 3))+'.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf4f326",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c2e034",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cdfb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path = '../data/covid-latent/'\n",
    "#data_path = '../data/covid-latent/undersampling/'\n",
    "#data_path = '../data/stance-detection-in-covid-19-tweets/stay_at_home_orders/' #face_masks, school_closures, stay_at_home_orders, fauci\n",
    "#data_path = '../data/russian-troll-tweets/'\n",
    "#data_path = '../data/COVIDSenti/'\n",
    "#data_path = '../data/birdwatch/'\n",
    "data_path = '../data/mediaeval22/old_task1/'\n",
    "\n",
    "filelist = os.listdir(data_path)\n",
    "\n",
    "\n",
    "df_list = [pd.read_csv(data_path+file) for file in filelist if 'fold' in file]\n",
    "\n",
    "\n",
    "test_df = df_list[k]\n",
    "\n",
    "train_df = pd.concat(df_list[:k]+df_list[k+1:])\n",
    "test_df = pd.concat([train_df, test_df])\n",
    "\n",
    "\n",
    "tw_train = train_df['tweet'].tolist()\n",
    "tw_test = test_df['tweet'].tolist()\n",
    "\n",
    "# tw_train = train_df['content'].tolist()\n",
    "# tw_test = test_df['content'].tolist()\n",
    "#ids_test = test_df['tweet'].tolist()\n",
    "\n",
    "\n",
    "if normalize_test_flag:\n",
    "    tw_train = normalize_text(tw_train)\n",
    "    tw_test = normalize_text(tw_test)\n",
    "\n",
    "#emotion\n",
    "train_df['emotion'][train_df['emotion'].isna()]='N'\n",
    "labels_train = train_df['emotion'].to_numpy()\n",
    "labels_train[labels_train=='N']=0\n",
    "labels_train[labels_train=='H']=1\n",
    "labels_train[labels_train=='A']=2\n",
    "labels_train[labels_train=='S']=3\n",
    "labels_train[labels_train=='F']=4\n",
    "labels_train = labels_train.tolist()\n",
    "\n",
    "#sentiment\n",
    "labels_train = train_df['label'].to_numpy()\n",
    "labels_train[labels_train=='neu']=1\n",
    "labels_train[labels_train=='pos']=2\n",
    "labels_train[labels_train=='neg']=0\n",
    "labels_train = labels_train.tolist()\n",
    "\n",
    "#political bias\n",
    "labels_train = train_df['account_category'].to_numpy()   \n",
    "labels_train[labels_train==\"Unknown\"]=1\n",
    "labels_train[labels_train==\"NonEnglish\"]=1\n",
    "labels_train[labels_train==\"Commercial\"]=1\n",
    "labels_train[labels_train==\"NewsFeed\"]=1\n",
    "labels_train[labels_train==\"HashtagGamer\"]=1\n",
    "labels_train[labels_train==\"Fearmonger\"]=1\n",
    "labels_train[labels_train==\"LeftTroll\"]=0\n",
    "labels_train[labels_train==\"RightTroll\"]=2\n",
    "labels_train = labels_train.tolist()\n",
    "\n",
    "#stance\n",
    "labels_train = train_df['Stance'].to_numpy()   \n",
    "labels_train[labels_train==\"FAVOR\"]=2\n",
    "labels_train[labels_train==\"NONE\"]=1\n",
    "labels_train[labels_train==\"AGAINST\"]=0\n",
    "labels_train = labels_train.tolist()\n",
    "\n",
    "#veracity\n",
    "labels_train = train_df['note'].to_numpy()   \n",
    "labels_train[labels_train==\"MISINFORMED_OR_POTENTIALLY_MISLEADING\"]=0\n",
    "labels_train[labels_train==\"NOT_MISLEADING\"]=1\n",
    "labels_train = labels_train.tolist()\n",
    "\n",
    "#conspiracy\n",
    "labels_train = train_df['conspiracy'].tolist()\n",
    "\n",
    "#emotion\n",
    "test_df['emotion'][test_df['emotion'].isna()]='N'\n",
    "labels_test = test_df['emotion'].to_numpy()\n",
    "labels_test[labels_test=='N']=0\n",
    "labels_test[labels_test=='H']=1\n",
    "labels_test[labels_test=='A']=2\n",
    "labels_test[labels_test=='S']=3\n",
    "labels_test[labels_test=='F']=4\n",
    "labels_test = labels_test.tolist()\n",
    "\n",
    "#sentiment\n",
    "labels_test = test_df['label'].to_numpy()\n",
    "labels_test[labels_test=='neu']=1\n",
    "labels_test[labels_test=='pos']=2\n",
    "labels_test[labels_test=='neg']=0\n",
    "labels_test = labels_test.tolist()\n",
    "\n",
    "#political bias\n",
    "labels_test = test_df['account_category'].to_numpy()   \n",
    "labels_test[labels_test==\"Unknown\"]=1\n",
    "labels_test[labels_test==\"NonEnglish\"]=1\n",
    "labels_test[labels_test==\"Commercial\"]=1\n",
    "labels_test[labels_test==\"NewsFeed\"]=1\n",
    "labels_test[labels_test==\"HashtagGamer\"]=1\n",
    "labels_test[labels_test==\"Fearmonger\"]=1\n",
    "labels_test[labels_test==\"LeftTroll\"]=0\n",
    "labels_test[labels_test==\"RightTroll\"]=2\n",
    "labels_test = labels_test.tolist()\n",
    "\n",
    "#stance\n",
    "labels_test = test_df['Stance'].to_numpy()   \n",
    "labels_test[labels_test==\"FAVOR\"]=2\n",
    "labels_test[labels_test==\"NONE\"]=1\n",
    "labels_test[labels_test==\"AGAINST\"]=0\n",
    "labels_test = labels_test.tolist()\n",
    "\n",
    "#veracity\n",
    "labels_test = test_df['note'].to_numpy()   \n",
    "labels_test[labels_test==\"MISINFORMED_OR_POTENTIALLY_MISLEADING\"]=0\n",
    "labels_test[labels_test==\"NOT_MISLEADING\"]=1\n",
    "labels_test = labels_test.tolist()\n",
    "\n",
    "#conspiracy\n",
    "labels_test = test_df['conspiracy'].tolist()\n",
    "\n",
    "\n",
    "ids_test = [i for i in range(0, len(test_df))]\n",
    "\n",
    "#labels_train = [[l-1 for l in L] for L in labels_train]\n",
    "#labels_test = [[l-1 for l in L] for L in labels_test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e16c764",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96750d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('../data/covid-latent/models/emotion_undersampling_CV0_e2_0.622.pth'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f2e166",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logits_full = []\n",
    "ground_truth_full = []\n",
    "ids_full = []\n",
    "\n",
    "eval_loss = 0\n",
    "steps=0\n",
    "for step, batch in enumerate(tqdm(test_dataloader)):\n",
    "\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    b_input_ids, b_input_mask, b_labels, b_token_type_ids, test_ids = batch\n",
    "\n",
    "    b_labels = b_labels.float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        logits = model(b_input_ids, b_token_type_ids, b_input_mask)\n",
    "        #loss = criterion(logits, b_labels.long())\n",
    "\n",
    "\n",
    "\n",
    "    logits = logits.detach().cpu().tolist()\n",
    "    logits_full.extend(logits)\n",
    "    ground_truth = b_labels.detach().cpu().tolist()\n",
    "    ground_truth_full.extend(ground_truth)\n",
    "    ids = test_ids.detach().cpu().tolist()\n",
    "    ids_full.extend(ids)\n",
    "    steps+=1\n",
    "    #eval_loss+=loss.detach().item()\n",
    "\n",
    "scheduler.step(eval_loss/steps)\n",
    "LOSS = eval_loss/steps\n",
    "F1 = metrics.f1_score(np.array(logits_full).argmax(axis=1), np.array(ground_truth_full), average='micro')\n",
    "ACC = metrics.accuracy_score(np.array(logits_full).argmax(axis=1), np.array(ground_truth_full))\n",
    "\n",
    "\n",
    "print(\"\\t Eval loss: {}\".format(LOSS))\n",
    "print(\"\\t Eval F1: {}\".format(F1))\n",
    "print(\"\\t Eval ACC: {}\".format(ACC))\n",
    "print(\"---\"*25)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b7bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "df['ids'] = ids_full\n",
    "df['emotion'] = np.array(logits_full).argmax(axis=1).tolist()\n",
    "#df.to_csv(data_path+'masks'+str(k)+'.csv', index=False)\n",
    "df.to_csv(data_path+'emotion_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ad449f",
   "metadata": {},
   "source": [
    "# Visu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8a0cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.zeros((3, 5))\n",
    "\n",
    "for i in trange(0, len(df)):\n",
    "    A[test_df['conspiracy'].tolist()[i]][df.sort_values(by='ids')['emotion'].tolist()[i]]+=1\n",
    "for i in range(0, 3):\n",
    "    A[i,:] = A[i,:]/A[i,:].sum()\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b422e71",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#NHASF\n",
    "#sns.light_palette(\"seagreen\", as_cmap=True)\n",
    "sns.heatmap(A, cmap = sns.light_palette(\"darkred\", as_cmap=True), yticklabels=['No Conspiracy', 'Discussing', 'Promoting'], xticklabels=['N', 'H', 'A', 'S', 'F'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
